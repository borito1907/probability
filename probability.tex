\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath, amsthm, amssymb}
\usepackage{parskip}
\newgeometry{vmargin={15mm}, hmargin={24mm,34mm}}
\theoremstyle{definition} 
\newtheorem{definition}{Definition}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[theorem]
\newtheorem{example}{Example}[theorem]

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Prob}{\mathcal{P}}
\newcommand{\fdiff}{f^{\prime}}
\newcommand{\interior}{\text{int}}
\newcommand{\dom}{\text{Dom}}
\newcommand{\ran}{\text{Ran}}
\newcommand{\expect}{\mathbb{E}}

\title{Probability Notes}
\date{April 2024}

\begin{document}

\maketitle

\begin{definition}
    Let $X$ be a set. $\Sigma \subseteq P(X)$ is a \textbf{$\sigma$-algebra} if it satisfies
    the following three properties:

    \begin{itemize}
        \item $X \in \Sigma$.
        \item $\Sigma$ is closed under complementation.
        \item $\Sigma$ is closed under countable unions.
    \end{itemize}
\end{definition}

\begin{lemma}
    Sigma algebras are closed under countable intersections.
\end{lemma}
\begin{proof}
    Follows by De Morgan's Laws.
\end{proof}

The smallest possible $\sigma$-algebra is $\{\varnothing, X\}$. It
is also the intersection of all possible $\sigma$-algebras on $X$.

The largest possible $\sigma$-algebra is $P(X)$.

Elements of $\sigma$-algebras are called \textbf{measurable sets}.

\begin{lemma}
    The intersection of a collection of $\sigma$-algebras is a $\sigma$-algebra.
\end{lemma}

\begin{lemma}
    For any collection $A$ of subsets of $\Omega$, there's a smallest $\sigma$-algebra
    containing $A$. This is called the \textbf{$\sigma$-algebra} generated by $A$ and denoted $\sigma(A)$.
\end{lemma}
\begin{proof}
    Let $S = \{ \mathcal{F} \subseteq P(X) : \mathcal{F} \text{is a $\sigma$-algebra containing } A\}$.
    We'll show that

    \[ \sigma(A) = \bigcap_{\mathcal{F} \in S} \mathcal{F}\]
    
    
\end{proof}


\begin{lemma}
    Let $\Omega = [0,1]$. Let $\Sigma$ be a $\sigma$-algebra that contains all closed intervals
    $[a,b]$ in $[0,1]$. Then, $\Sigma$ contains all open and closed sets in $[0,1]$.

    In particular, the $\sigma$-algebra generated by all open sets is equivalent to $\Sigma$.
\end{lemma}
\begin{proof}
    It suffices to show that $\Sigma$ contains all open sets since closed sets are just complements
    of open sets and $\Sigma$ is closed under complementation.

    Since open intervals are a basis for the standard topology on $[0,1]$, it suffices to show that
    $\Sigma$ contains all open intervals.

    Let $(a,b) \subseteq [0,1]$. \textit{Rest of the argument is straightforward.}
\end{proof}



\begin{definition}
    If $X$ is a metric/topological space, the $\sigma$-algebra generated by all open sets
    is called a \textbf{Borel $\sigma$-algebra}.
\end{definition}



\newpage

When is office hours?

\begin{enumerate}
    \item $\Pr[\Omega] = 1$
    \item $\forall A \in \mathcal{F}: \Pr[A] \geq 0$
    \item Let $A_{n}$ be a disjoint sequence of events. Then, $\sum_{n = 1}^{\infty} \Pr(A_{n}) = \Pr(\bigcup_{n = 1}^{\infty} A_{n})$
\end{enumerate}

If you replace the countable additivity axiom with a finite additivity axiom, 
things go crazy. Uncountable additivity also doesn't work, things break.

\begin{definition}
    A \textbf{random variable} is a function $X:\Omega \xrightarrow{} \R$ that
    satisfies

    \[\forall \lambda \in \R: \{\omega \in \Omega: X(\omega) \leq \lambda\} \in \mathcal{F}\]
\end{definition}

Since $\mathcal{F}$ is closed under complements, equivalently,
we can require $\forall \lambda \in \R: \{\omega \in \Omega: X(\omega) > \lambda\} \in \mathcal{F}$.

\begin{lemma}
    Let $X,Y$ be random variables. Then, $X + Y$ defined by 
    pointwise addition is a random variable.
\end{lemma}
\begin{proof}
    The fact that $X+Y$ is a function is immediate. Thus, it suffices to show that
    $\forall \lambda \in \R: \{\omega \in \Omega: X(\omega) + Y(\omega)> \lambda\} \in \mathcal{F}$.

    What we'd like to have is the following:

    \[ \{ X + Y > \lambda\} = \bigcup_{\mu \in \R}(\{X > \mu\} \cap \{Y > \lambda - \mu\}) \]

    This is true, but not helpful, since the uncountable union doesn't help us. We'll use the density
    of the rationals to bring the cardinality down. We'd like to show the following:

    \[ \{ X + Y > \lambda\} = \bigcup_{q \in \Q}(\{X > q\} \cap \{Y > \lambda - q\}) \]

    Let's show the containment both ways.

\end{proof}

\begin{definition}
    The \textbf{cumulative distribution function (cdf)} of a random variable
    $X: \Omega \xrightarrow{} \R$ is defined to be $F_{X}: \R \xrightarrow{} [0,1]$
    by

    \[ F_{X}(x) = \Pr(\{w \in \Omega: X(\omega) \leq x\})\]
\end{definition}

The cdf characterizes the distribution of $X$.

We say that $X \sim Bernoulli(p)$ to mean "is distributed as".

In this case:

% \begin{cases}
%     0 if x < 0
%     1 - p if 0 \leq x < 1
%     1 if x \geq 1
% \end{cases}

\begin{definition}
    A function $f$ is \textbf{cadlag} if it's continuous from the right and limits
    exist from the left. More formally,

    \begin{itemize}
        \item 
    \end{itemize}
\end{definition}

\begin{proposition}
    A function $F: \R \xrightarrow{} [0,1]$ is the cumulative distribution
    function of a random variable if and only if
    
    \begin{itemize}
        \item $\lim_{x \to -\infty} F(x) = 0$
        \item $\lim_{x \to \infty} F(x) = 1$
        \item It is cadlag. (It is continuous from the right snd limits exist from the left.)
    \end{itemize}
\end{proposition}

Professor calls this the monotone convergence of probabilities. 

If $A_{1} \supseteq A_{2} \supseteq A_{3}$, then

Then, $\Pr(\bigcap A_{n}) = \lim_{n \to \infty} \Pr(A_{n})$.

If $A_{1} \subseteq A_{2} \subseteq A_{3}$ ..., then

Then, $\Pr(\bigcup A_{n}) = \lim_{n \to \infty} \Pr(A_{n})$.

These two properties are equivalent to the countable additivity of countable additivity of probability measures.

\begin{itemize}
    \item If $F$ has these three properties, then we can build a random variable
    with this CFG -- e.g. the inverse distribution method.
    \item If $X$ is a random variable, then $\lim_{y \to x^{-}} F_{x}(y) = \Pr(X < x)$.
    NOT LESS THAN OR EQUAL TO--LESS THAN.
\end{itemize}

\newpage

\begin{definition}
    A \textbf{discrete random variable X} on the probability space $(\Omega, \mathcal{F}, \Prob)$
    is a mapping $X: \Omega \xrightarrow{} \R$ such that

    \begin{itemize}
        \item $X(\Omega)$ is at most countable.
        \item $\forall x \in \R: \{w \in \Omega: X(w) = x\} = X^{-1}(x) \in \mathcal{F}$.
    \end{itemize}
\end{definition}

\begin{definition}
    The \textbf{probability mass function (pmf)} of the discrete random variable X is
    the function $p_{X}:\R \xrightarrow{} [0,1]$ defined by $p_{X}(x) = \Prob(X^{-1}(x))$.
\end{definition}

\newpage

\section{Birthday Paradox}

\newpage

\section{Geometric Random Variables}

\begin{definition}
    $X$ has a \textbf{geometric distribution} if $\forall k \in \Z^{+}: \Prob(X = k) = (1-p)^{k-1}p$ for some $p \in (0,1)$.
\end{definition}

Let's now examine the probability that $X \sim Geometric(p)$ is even.



\newpage

\section{Bernoulli Random Variables}

\begin{definition}
    $X$ has a \textbf{Bernoulli distribution} if $\Prob(X = 0) = p$ and $\Prob(X = 1) = 1 - p$ for some $p \in [0,1]$.
\end{definition}

\textbf{I carry out a sequence of independent Bernoulli($p$) trials. Independently of me,
you carry out a sequence of independent Bernoulli($q$) trials. Here $p,q \in (0,1)$.
What is the probability we both have our first success after the same number of
trials?}

\newpage

\section{Binomial Distribution}



\newpage

\section{Variance}

Unlike the expectation, the variance of a random variable violates both conditions of linearity.

We have that

\[ var(cX) = c^{2}var(X)\]



\newpage

\section{Exponential Random Variable}

\newpage

\section{Poisson Distribution}

\begin{definition}
    $X$ has a \textbf{Poisson distribution} with parameter $\lambda > 0$ if $X$ takes values
    in $\Z^{+}$ and \[\Prob(X = k) = \frac{1}{k!}\lambda^{k}e^{-\lambda}\].
\end{definition}



\newpage

\section{Normal (Gaussian) Distributions}

\[ f(z) = \frac{1}{2\pi}e^{-z^{2}/2}\]

To find the normalizing constant, we can use a trick with multivariable integration and
polar coordinates. The point is that while switching to polar coordinates, a factor of $r$
pops out due to the Jacobian which makes the integral easier.

It's in Stirling's approximation in my Notability notes. In fact, it's provably
impossible to otherwise evaluate the indefinite integral with the regular techniques and elementary
functions in closed form.

Notice that all moments of the normal distribution are 0 since the probability density function
of the normal distribution is an odd function.

Then, we can define $X \sim N(\mu, \sigma^{2})$ if $X = \mu + \sigma Y$ where $Y$ is the
standard normal distribution.

We can also reverse this and "standardize" a normal variable. Given $Y \sim N(\mu, \sigma^{2})$,
we can define the standard normal $Z = \frac{Y - \mu}{\sigma}$. Also notice that standardization
gets rid of the units so interpreting it is straightforward.



\newpage

\section{Limsup and Liminf for Events}

Let $E_{n}$ be a sequence of events.

\begin{lemma}
    \[\{ \text{infinitely many $E_{k}$ occur}\} = \{ \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} E_{k}\}\]
\end{lemma}
\begin{proof}
    
\end{proof}

This event is called $\limsup_{k \to \infty}E_{k}$.



\newpage

\section{Expectation}

\begin{lemma}
    Let $X: \Omega \xrightarrow{} \Z$ be a \textit{non-negative} random variable. Then,

    \[  \expect(X) = \sum_{n = 1}^{\infty} \Prob(X \geq n) \]

    Therefore $\expect(X)$ exists if and only if the sum on the right-hand side converges.
\end{lemma}
\begin{proof}
    This is a simple rearrangement of sums, which converges to the same value because of absolute convergence.
\end{proof}

\subsection{Problems}

\textbf{I play the following game using a coin that lands heads with probability p. I
start with $X_{0} = 1$ and at each stage I gamble all I have on the toss of the coin.
If it lands heads I end up with twice what I started with; if it lands tails I lose
everything. All coin tosses are statistically independent.}

\newpage

\section{Problems}

\textbf{Starting at the origin on the line, I take a step of one unit to the left or to
the right with probability 1/2. I do this repeatedly with independent steps. If I
take 2n steps, what is the probability that I find myself back at the origin?}

Let $X \sim Binomial(2n,1/2)$. We want to calculate $\Prob(X = n)$. The rest is
straightforward calculation.

\textbf{You and I independently complete this same random walk. What is the
probability that we end up in the same location?}


\textbf{Suppose my knowledge/ignorance of the number of branches of a certain store (in
my city) is given by the following probability law: }

\newpage

\section{Poisson Processes}

What if we want to compute the time of the nth success? Is there a probability distribution that models this?
It turns out that this will be the Gamma distribution, i.e. the nth success will be modeled using $Gamma(n,\lambda)$.

\newpage

\section{Central Limit Theorem}

Intuitively, here is the idea: the sum of n identical independently distributed 
random variables approach the normal distribution as n goes to infinity.

\subsection{Resources}

\begin{itemize}
    \item 3Blue1Brown video
\end{itemize}

\newpage

\section{The Moment Problem}



Note that some distributions (which happen to have all moments finite) are determined
by finitely many moments. This is because the finitely many moments also uniquely determine
the rest of the moment sequence.

\begin{example}
    For example, if $\mathcal{E}(X^{2}) = 0$, then $X$ is a "constant" random variable.
\end{example}

\begin{example}
    For example, if $\mathcal{E}(Y^{2}(1-Y)^{2}) = 0$ and $\mathcal{E}(Y) = p$, $Y$ is
    a Bernoulli random variable with parameter $p$ since $f(x) = x^{2}(1-x)^{2}$ only
    takes positive values and is only $0$ at $0$ and $1$.
\end{example}

\begin{theorem}
    If finitely many moments determine $F_{X}$, $X$ takes finitely many moments.
\end{theorem}

The proof is far beyond what we're capable of right now.

\begin{theorem}
    Suppose $m_{k} = \mathcal{E}(X^{k})$ satisfy that for some $a > 0$ the series 

    \[ \sum_{k = 0}^{\infty} \frac{a^{2k}m_{2k}}{(2k)!}\]

    Then, $\forall k \in \Z^{\geq 0}: \mathcal{E}(X^{k}) = \mathcal{E}(Y^{k})$ implies $F_{X} = F_{Y}$.
\end{theorem}
\begin{proof}
    Using $\lvert x^{2k + 1} \rvert \leq 1 + \lvert x ^{2k + 2} \rvert$, we see that (*) guarantees
    the convergence of 

    \[ \phi_{X}(\xi) = \mathcal{E}(e^{i \xi X}) = \sum_{k = 0}^{\infty} \frac{(i\xi)^{k}m_{k}}{(k)!} \]

    for all $\lvert xi \rvert \leq a$.

    Thus, moments satisfying (*) tell us $\phi_{X}(\xi)$ but only for small $\lvert \xi \rvert$.

    However, knowing this for small $\xi$ is not enough. WHY? UNDERSTAND THIS.

    What follows is usually a dumb idea, but it works in this case. Let's try to find the power series
    of $\phi_{X}(\xi)$ about $\xi_{0} = \pm a/2$.

    Well,

    
\end{proof}

\newpage

\section{Chicken-Egg Problem}

Galton-Watson Processes



\end{document}